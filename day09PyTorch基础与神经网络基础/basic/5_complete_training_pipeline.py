# 4_complete_training_pipeline.py
print("=== ç¬¬4æ­¥ï¼šå®Œæ•´æ·±åº¦å­¦ä¹ è®­ç»ƒæµç¨‹ ===")
print("ä»æ•°æ®å‡†å¤‡åˆ°æ¨¡å‹éƒ¨ç½²çš„å®Œæ•´é¡¹ç›®")

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import time
import os

# è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
torch.manual_seed(42)
np.random.seed(42)

print("ğŸ”§ 1. æ•°æ®å‡†å¤‡å’Œé¢„å¤„ç†")

# 1.1 åˆ›å»ºåˆæˆæ•°æ®é›†ï¼ˆæ¨¡æ‹ŸçœŸå®æ•°æ®ï¼‰
print("\nğŸ“Š ç”Ÿæˆåˆæˆæ•°æ®é›†...")
X, y = make_classification(
    n_samples=1000,  # 1000ä¸ªæ ·æœ¬
    n_features=20,  # 20ä¸ªç‰¹å¾
    n_informative=15,  # 15ä¸ªæœ‰ç”¨ç‰¹å¾
    n_redundant=5,  # 5ä¸ªå†—ä½™ç‰¹å¾
    n_classes=3,  # 3ä¸ªç±»åˆ«
    n_clusters_per_class=1,  # æ¯ä¸ªç±»åˆ«1ä¸ªç°‡
    random_state=42
)

print(f"æ•°æ®é›†å½¢çŠ¶: X{X.shape}, y{y.shape}")
print(f"ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y)}")

# 1.2 æ•°æ®æ ‡å‡†åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 1.3 åˆ’åˆ†æ•°æ®é›†
X_train, X_temp, y_train, y_temp = train_test_split(
    X_scaled, y, test_size=0.3, random_state=42, stratify=y
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
)

print(f"\nğŸ“‹ æ•°æ®é›†åˆ’åˆ†:")
print(f"  è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬")
print(f"  éªŒè¯é›†: {X_val.shape[0]} æ ·æœ¬")
print(f"  æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬")


# 1.4 åˆ›å»ºPyTorchæ•°æ®é›†ç±»
class ClassificationDataset(Dataset):
    def __init__(self, features, labels):
        self.features = torch.tensor(features, dtype=torch.float32)
        self.labels = torch.tensor(labels, dtype=torch.long)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]


# åˆ›å»ºæ•°æ®é›†å®ä¾‹
train_dataset = ClassificationDataset(X_train, y_train)
val_dataset = ClassificationDataset(X_val, y_val)
test_dataset = ClassificationDataset(X_test, y_test)

# 1.5 åˆ›å»ºæ•°æ®åŠ è½½å™¨
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

print(f"\nğŸ“¦ æ•°æ®åŠ è½½å™¨:")
print(f"  è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}")
print(f"  éªŒè¯æ‰¹æ¬¡: {len(val_loader)}")
print(f"  æµ‹è¯•æ‰¹æ¬¡: {len(test_loader)}")

print("\nğŸ”§ 2. å®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹")


class AdvancedClassifier(nn.Module):
    """æ›´å¤æ‚çš„ç¥ç»ç½‘ç»œåˆ†ç±»å™¨"""

    def __init__(self, input_size=20, hidden_sizes=[64, 32], num_classes=3, dropout_rate=0.3):
        super(AdvancedClassifier, self).__init__()

        # æ„å»ºéšè—å±‚
        layers = []
        prev_size = input_size

        for i, hidden_size in enumerate(hidden_sizes):
            layers.append(nn.Linear(prev_size, hidden_size))
            layers.append(nn.BatchNorm1d(hidden_size))  # æ‰¹å½’ä¸€åŒ–ï¼ŒåŠ é€Ÿè®­ç»ƒ
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout_rate))  # é˜²æ­¢è¿‡æ‹Ÿåˆ
            prev_size = hidden_size

        # è¾“å‡ºå±‚
        layers.append(nn.Linear(prev_size, num_classes))

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)


# åˆ›å»ºæ¨¡å‹å®ä¾‹
model = AdvancedClassifier(input_size=20, hidden_sizes=[128, 64, 32], num_classes=3)
print(f"\nğŸ§  æ¨¡å‹ç»“æ„:")
print(model)

# è®¡ç®—å‚æ•°æ•°é‡
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"\nğŸ“Š å‚æ•°ç»Ÿè®¡:")
print(f"  æ€»å‚æ•°: {total_params:,}")
print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

print("\nğŸ”§ 3. å®šä¹‰è®­ç»ƒç»„ä»¶")

# 3.1 æ£€æŸ¥GPUå¯ç”¨æ€§
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")

model = model.to(device)

# 3.2 å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # æ ‡ç­¾å¹³æ»‘é˜²æ­¢è¿‡æ‹Ÿåˆ
optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)

# 3.3 å­¦ä¹ ç‡è°ƒåº¦å™¨
scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)

print(f"\nâš™ï¸ è®­ç»ƒé…ç½®:")
print(f"  æŸå¤±å‡½æ•°: CrossEntropyLoss")
print(f"  ä¼˜åŒ–å™¨: AdamW")
print(f"  åˆå§‹å­¦ä¹ ç‡: 0.001")
print(f"  æƒé‡è¡°å‡: 1e-4")

print("\nğŸ”§ 4. å®šä¹‰è®­ç»ƒå’Œè¯„ä¼°å‡½æ•°")


def train_epoch(model, dataloader, criterion, optimizer, device):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for batch_idx, (data, target) in enumerate(dataloader):
        data, target = data.to(device), target.to(device)

        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        outputs = model(data)
        loss = criterion(outputs, target)

        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()

        # ç»Ÿè®¡
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += target.size(0)
        correct += predicted.eq(target).sum().item()

    epoch_loss = running_loss / len(dataloader)
    epoch_acc = 100. * correct / total

    return epoch_loss, epoch_acc


def evaluate(model, dataloader, criterion, device):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for data, target in dataloader:
            data, target = data.to(device), target.to(device)
            outputs = model(data)
            loss = criterion(outputs, target)

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()

    epoch_loss = running_loss / len(dataloader)
    epoch_acc = 100. * correct / total

    return epoch_loss, epoch_acc


print("\nğŸ”§ 5. å¼€å§‹è®­ç»ƒ")

# è®°å½•è®­ç»ƒå†å²
history = {
    'train_loss': [], 'train_acc': [],
    'val_loss': [], 'val_acc': [],
    'learning_rates': []
}

# æ—©åœè®¾ç½®
patience = 10
patience_counter = 0
best_val_acc = 0.0
best_model_state = None

num_epochs = 50
start_time = time.time()

print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ {num_epochs} ä¸ªepoch...")
print("-" * 60)

for epoch in range(num_epochs):
    epoch_start = time.time()

    # è®­ç»ƒ
    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)

    # éªŒè¯
    val_loss, val_acc = evaluate(model, val_loader, criterion, device)

    # è°ƒæ•´å­¦ä¹ ç‡
    scheduler.step()
    current_lr = optimizer.param_groups[0]['lr']

    # è®°å½•å†å²
    history['train_loss'].append(train_loss)
    history['train_acc'].append(train_acc)
    history['val_loss'].append(val_loss)
    history['val_acc'].append(val_acc)
    history['learning_rates'].append(current_lr)

    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        best_model_state = model.state_dict().copy()
        patience_counter = 0
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_acc': val_acc,
            'train_acc': train_acc,
        }, 'best_model.pth')
    else:
        patience_counter += 1

    # æ‰“å°è¿›åº¦
    epoch_time = time.time() - epoch_start
    print(f'Epoch {epoch + 1:2d}/{num_epochs} | '
          f'è®­ç»ƒæŸå¤±: {train_loss:.4f} | è®­ç»ƒå‡†ç¡®ç‡: {train_acc:6.2f}% | '
          f'éªŒè¯æŸå¤±: {val_loss:.4f} | éªŒè¯å‡†ç¡®ç‡: {val_acc:6.2f}% | '
          f'å­¦ä¹ ç‡: {current_lr:.6f} | æ—¶é—´: {epoch_time:.1f}s')

    # æ—©åœæ£€æŸ¥
    if patience_counter >= patience:
        print(f"\nâ¹ï¸  æ—©åœè§¦å‘: éªŒè¯å‡†ç¡®ç‡ {patience} ä¸ªepochæœªæå‡")
        break

# åŠ è½½æœ€ä½³æ¨¡å‹
model.load_state_dict(best_model_state)
total_time = time.time() - start_time

print(f"\nâœ… è®­ç»ƒå®Œæˆ! æ€»æ—¶é—´: {total_time:.1f}ç§’")
print(f"ğŸ† æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")

print("\nğŸ”§ 6. è®­ç»ƒç»“æœå¯è§†åŒ–")

# åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

# 6.1 æŸå¤±æ›²çº¿
axes[0, 0].plot(history['train_loss'], label='è®­ç»ƒæŸå¤±', linewidth=2, color='blue')
axes[0, 0].plot(history['val_loss'], label='éªŒè¯æŸå¤±', linewidth=2, color='red')
axes[0, 0].set_xlabel('Epoch')
axes[0, 0].set_ylabel('Loss')
axes[0, 0].set_title('æŸå¤±æ›²çº¿')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# 6.2 å‡†ç¡®ç‡æ›²çº¿
axes[0, 1].plot(history['train_acc'], label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2, color='blue')
axes[0, 1].plot(history['val_acc'], label='éªŒè¯å‡†ç¡®ç‡', linewidth=2, color='red')
axes[0, 1].axhline(y=best_val_acc, color='green', linestyle='--',
                   label=f'æœ€ä½³: {best_val_acc:.1f}%')
axes[0, 1].set_xlabel('Epoch')
axes[0, 1].set_ylabel('Accuracy (%)')
axes[0, 1].set_title('å‡†ç¡®ç‡æ›²çº¿')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# 6.3 å­¦ä¹ ç‡å˜åŒ–
axes[0, 2].plot(history['learning_rates'], linewidth=2, color='green')
axes[0, 2].set_xlabel('Epoch')
axes[0, 2].set_ylabel('Learning Rate')
axes[0, 2].set_title('å­¦ä¹ ç‡å˜åŒ–')
axes[0, 2].grid(True, alpha=0.3)

# 6.4 è®­ç»ƒç»Ÿè®¡
axes[1, 0].axis('off')
stats_text = (
    f"è®­ç»ƒç»Ÿè®¡\n\n"
    f"è®­ç»ƒæ—¶é—´: {total_time:.1f}ç§’\n"
    f"æ€»Epochæ•°: {len(history['train_loss'])}\n"
    f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%\n"
    f"æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {history['train_acc'][-1]:.2f}%\n"
    f"æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {history['val_acc'][-1]:.2f}%\n"
    f"æ‰¹æ¬¡å¤§å°: {batch_size}\n"
    f"æ¨¡å‹å‚æ•°: {trainable_params:,}"
)
axes[1, 0].text(0.5, 0.5, stats_text, ha='center', va='center', fontsize=12,
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

# 6.5 æ··æ·†çŸ©é˜µï¼ˆç®€åŒ–ç‰ˆï¼‰
axes[1, 1].axis('off')
# è¿™é‡Œå¯ä»¥æ·»åŠ çœŸæ­£çš„æ··æ·†çŸ©é˜µï¼Œä¸ºä¿æŒç®€å•å…ˆç•™ç©º

# 6.6 ç‰¹å¾é‡è¦æ€§ï¼ˆç®€åŒ–ç‰ˆï¼‰
axes[1, 2].axis('off')
# è¿™é‡Œå¯ä»¥æ·»åŠ ç‰¹å¾é‡è¦æ€§åˆ†æ

plt.suptitle('æ·±åº¦å­¦ä¹ è®­ç»ƒç»“æœåˆ†æ', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

print("\nğŸ”§ 7. æ¨¡å‹æµ‹è¯•å’Œè¯„ä¼°")

# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ä½³æ¨¡å‹
test_loss, test_acc = evaluate(model, test_loader, criterion, device)

print(f"\nğŸ“Š æµ‹è¯•é›†ç»“æœ:")
print(f"  æµ‹è¯•æŸå¤±: {test_loss:.4f}")
print(f"  æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")

# è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
model.eval()
all_predictions = []
all_targets = []

with torch.no_grad():
    for data, target in test_loader:
        data, target = data.to(device), target.to(device)
        outputs = model(data)
        _, predicted = outputs.max(1)

        all_predictions.extend(predicted.cpu().numpy())
        all_targets.extend(target.cpu().numpy())

# è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
from sklearn.metrics import classification_report

print(f"\nğŸ“ˆ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(all_targets, all_predictions,
                            target_names=['ç±»åˆ« 0', 'ç±»åˆ« 1', 'ç±»åˆ« 2']))

print("\nğŸ”§ 8. æ¨¡å‹ä¿å­˜å’Œéƒ¨ç½²")

# 8.1 ä¿å­˜å®Œæ•´æ¨¡å‹
torch.save(model, 'complete_model.pth')
print("âœ… å®Œæ•´æ¨¡å‹å·²ä¿å­˜ä¸º 'complete_model.pth'")

# 8.2 ä¿å­˜æ¨¡å‹çŠ¶æ€ï¼ˆæ¨èæ–¹å¼ï¼‰
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'scaler_mean': scaler.mean_,
    'scaler_scale': scaler.scale_,
    'input_size': 20,
    'hidden_sizes': [128, 64, 32],
    'num_classes': 3,
    'test_accuracy': test_acc
}, 'model_checkpoint.pth')
print("âœ… æ¨¡å‹æ£€æŸ¥ç‚¹å·²ä¿å­˜ä¸º 'model_checkpoint.pth'")

# 8.3 ä¿å­˜ä¸ºTorchScriptï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰
scripted_model = torch.jit.script(model)
scripted_model.save('model_scripted.pt')
print("âœ… TorchScriptæ¨¡å‹å·²ä¿å­˜ä¸º 'model_scripted.pt'")

print("\nğŸ”§ 9. æ¨¡å‹åŠ è½½å’Œæ¨ç†ç¤ºä¾‹")


# æ¼”ç¤ºå¦‚ä½•åŠ è½½å’Œä½¿ç”¨æ¨¡å‹
def load_and_predict(model_path, input_data):
    """åŠ è½½æ¨¡å‹å¹¶è¿›è¡Œé¢„æµ‹"""
    # åŠ è½½æ¨¡å‹
    checkpoint = torch.load(model_path, map_location=device)

    # åˆ›å»ºæ–°æ¨¡å‹å®ä¾‹
    loaded_model = AdvancedClassifier(
        input_size=checkpoint['input_size'],
        hidden_sizes=checkpoint['hidden_sizes'],
        num_classes=checkpoint['num_classes']
    )

    # åŠ è½½æƒé‡
    loaded_model.load_state_dict(checkpoint['model_state_dict'])
    loaded_model.to(device)
    loaded_model.eval()

    # æ•°æ®é¢„å¤„ç†ï¼ˆä½¿ç”¨ä¿å­˜çš„scalerå‚æ•°ï¼‰
    scaler_mean = checkpoint['scaler_mean']
    scaler_scale = checkpoint['scaler_scale']
    input_scaled = (input_data - scaler_mean) / scaler_scale

    # é¢„æµ‹
    with torch.no_grad():
        input_tensor = torch.tensor(input_scaled, dtype=torch.float32).to(device)
        output = loaded_model(input_tensor)
        probabilities = torch.softmax(output, dim=1)
        prediction = torch.argmax(output, dim=1)

    return prediction.cpu().numpy(), probabilities.cpu().numpy()


# æµ‹è¯•åŠ è½½çš„æ¨¡å‹
print(f"\nğŸ§ª æµ‹è¯•æ¨¡å‹åŠ è½½å’Œé¢„æµ‹:")
test_sample = X_test[0:1]  # å–ç¬¬ä¸€ä¸ªæµ‹è¯•æ ·æœ¬
prediction, probabilities = load_and_predict('model_checkpoint.pth', test_sample)

print(f"è¾“å…¥æ ·æœ¬å½¢çŠ¶: {test_sample.shape}")
print(f"çœŸå®æ ‡ç­¾: {y_test[0]}")
print(f"é¢„æµ‹æ ‡ç­¾: {prediction[0]}")
print(f"é¢„æµ‹æ¦‚ç‡: {probabilities[0].round(3)}")
print(f"é¢„æµ‹æ˜¯å¦æ­£ç¡®: {'âœ…' if prediction[0] == y_test[0] else 'âŒ'}")

print("\nğŸ”§ 10. åˆ›å»ºé¢„æµ‹å‡½æ•°")


def predict_new_sample(model, scaler, sample, class_names=None):
    """å¯¹æ–°æ ·æœ¬è¿›è¡Œé¢„æµ‹çš„ä¾¿æ·å‡½æ•°"""
    if class_names is None:
        class_names = ['ç±»åˆ« 0', 'ç±»åˆ« 1', 'ç±»åˆ« 2']

    # æ•°æ®é¢„å¤„ç†
    sample_scaled = scaler.transform(sample.reshape(1, -1))
    sample_tensor = torch.tensor(sample_scaled, dtype=torch.float32).to(device)

    # é¢„æµ‹
    model.eval()
    with torch.no_grad():
        output = model(sample_tensor)
        probabilities = torch.softmax(output, dim=1)
        prediction = torch.argmax(output, dim=1)

    pred_class = prediction.item()
    confidence = probabilities[0][pred_class].item()

    print(f"\nğŸ”® é¢„æµ‹ç»“æœ:")
    print(f"  é¢„æµ‹ç±»åˆ«: {class_names[pred_class]} (ç´¢å¼•: {pred_class})")
    print(f"  ç½®ä¿¡åº¦: {confidence:.3f}")
    print(f"  æ‰€æœ‰ç±»åˆ«æ¦‚ç‡:")
    for i, prob in enumerate(probabilities[0]):
        class_name = class_names[i] if class_names else f'ç±»åˆ« {i}'
        print(f"    {class_name}: {prob:.3f}")

    return pred_class, confidence


# æµ‹è¯•æ–°æ ·æœ¬é¢„æµ‹
print(f"\nğŸ¯ æ–°æ ·æœ¬é¢„æµ‹æ¼”ç¤º:")
new_sample = np.random.randn(20)  # éšæœºç”Ÿæˆä¸€ä¸ªæ ·æœ¬
predict_new_sample(model, scaler, new_sample)

print("\n" + "=" * 60)
print("ğŸ‰ å®Œæ•´æ·±åº¦å­¦ä¹ æµç¨‹å®Œæˆï¼")
print("=" * 60)
print("\nğŸ“š å­¦ä¹ æ€»ç»“:")
print("âœ… 1. æ•°æ®å‡†å¤‡å’Œé¢„å¤„ç†")
print("âœ… 2. ç¥ç»ç½‘ç»œæ¨¡å‹è®¾è®¡")
print("âœ… 3. è®­ç»ƒå¾ªç¯å®ç°")
print("âœ… 4. æ¨¡å‹è¯„ä¼°å’ŒéªŒè¯")
print("âœ… 5. æ¨¡å‹ä¿å­˜å’ŒåŠ è½½")
print("âœ… 6. æ–°æ ·æœ¬é¢„æµ‹")
print("\nğŸš€ ä¸‹ä¸€æ­¥: å°è¯•ä¿®æ”¹å‚æ•°ï¼Œè§‚å¯Ÿå¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼")
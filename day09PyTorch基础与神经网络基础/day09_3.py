"""
ç¬¬9å¤©ï¼šPyTorchå®Œæ•´å®æˆ˜ï¼ˆå¯è¿è¡Œç‰ˆæœ¬ï¼‰
æ¯ä¸€æ®µä»£ç éƒ½å¯ä»¥ç‹¬ç«‹è¿è¡Œ
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader, TensorDataset

print("ğŸ”¥ PyTorchç‰ˆæœ¬:", torch.__version__)
print("ğŸ”¥ CUDAå¯ç”¨:", torch.cuda.is_available())


# ============================================================================
# 1. å¼ é‡åŸºç¡€æ“ä½œï¼ˆå®Œæ•´å¯è¿è¡Œï¼‰
# ============================================================================

def tensor_basics_demo():
    print("\n" + "=" * 60)
    print("1. å¼ é‡åŸºç¡€æ“ä½œ")
    print("=" * 60)

    # åˆ›å»ºå¼ é‡
    x = torch.tensor([1.0, 2.0, 3.0])
    y = torch.zeros(2, 3)
    z = torch.ones(3, 2)

    print("åŸºæœ¬å¼ é‡åˆ›å»º:")
    print(f"x = {x}")
    print(f"y (2x3é›¶çŸ©é˜µ) = \n{y}")
    print(f"z (3x2ä¸€çŸ©é˜µ) = \n{z}")

    # å¼ é‡è¿ç®—
    a = torch.tensor([1.0, 2.0])
    b = torch.tensor([3.0, 4.0])

    print("\nå¼ é‡è¿ç®—:")
    print(f"a + b = {a + b}")
    print(f"a * b = {a * b}")
    print(f"ç‚¹ç§¯ = {torch.dot(a, b)}")

    # çŸ©é˜µä¹˜æ³•
    matrix1 = torch.tensor([[1.0, 2.0], [3.0, 4.0]])
    matrix2 = torch.tensor([[5.0, 6.0], [7.0, 8.0]])
    result = torch.matmul(matrix1, matrix2)
    print(f"çŸ©é˜µä¹˜æ³•: \n{result}")

    # å½¢çŠ¶æ“ä½œ
    tensor_2d = torch.randn(2, 3)
    print(f"\nå½¢çŠ¶æ“ä½œ:")
    print(f"åŸå§‹: {tensor_2d.shape}")
    print(f"è½¬ç½®: {tensor_2d.t().shape}")
    print(f"é‡å¡‘ä¸º3x2: {tensor_2d.view(3, 2).shape}")


# è¿è¡Œç¬¬ä¸€æ®µ
tensor_basics_demo()


# ============================================================================
# 2. è‡ªåŠ¨æ±‚å¯¼ç³»ç»Ÿï¼ˆå®Œæ•´å¯è¿è¡Œï¼‰
# ============================================================================

def autograd_demo():
    print("\n" + "=" * 60)
    print("2. è‡ªåŠ¨æ±‚å¯¼ç³»ç»Ÿ")
    print("=" * 60)

    # ç®€å•çº¿æ€§å‡½æ•°æ±‚å¯¼
    x = torch.tensor(2.0, requires_grad=True)
    w = torch.tensor(3.0, requires_grad=True)
    b = torch.tensor(1.0, requires_grad=True)

    y = w * x + b  # y = 3 * 2 + 1 = 7
    y.backward()

    print("ç®€å•çº¿æ€§å‡½æ•°:")
    print(f"y = {y.item()}")
    print(f"dy/dx = {x.grad}")  # åº”è¯¥æ˜¯3
    print(f"dy/dw = {w.grad}")  # åº”è¯¥æ˜¯2
    print(f"dy/db = {b.grad}")  # åº”è¯¥æ˜¯1

    # å¤æ‚å‡½æ•°æ±‚å¯¼
    x2 = torch.tensor(2.0, requires_grad=True)
    y2 = x2 ** 2 + 3 * x2 + 1
    y2.backward()

    print(f"\nå¤æ‚å‡½æ•° y = xÂ² + 3x + 1:")
    print(f"å½“x=2æ—¶, y = {y2.item()}")
    print(f"å¯¼æ•° dy/dx = {x2.grad}")  # 2 * 2 + 3 = 7


# è¿è¡Œç¬¬äºŒæ®µ
autograd_demo()


# ============================================================================
# 3. è‡ªå®šä¹‰ç¥ç»ç½‘ç»œï¼ˆå®Œæ•´å¯è¿è¡Œï¼‰
# ============================================================================

class SimpleNet(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, output_size)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x


def network_demo():
    print("\n" + "=" * 60)
    print("3. ç¥ç»ç½‘ç»œæ¨¡å—")
    print("=" * 60)

    # åˆ›å»ºç½‘ç»œ
    model = SimpleNet(input_size=10, hidden_size=20, output_size=3)

    print("ç½‘ç»œç»“æ„:")
    print(model)

    # å‚æ•°ç»Ÿè®¡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"\næ€»å‚æ•°æ•°é‡: {total_params:,}")

    # å‰å‘ä¼ æ’­æ¼”ç¤º
    batch_size = 4
    x = torch.randn(batch_size, 10)
    output = model(x)

    print(f"\nè¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")

    # æŸ¥çœ‹å„å±‚å‚æ•°
    print("\nå„å±‚å‚æ•°å½¢çŠ¶:")
    for name, param in model.named_parameters():
        print(f"{name}: {param.shape}")


# è¿è¡Œç¬¬ä¸‰æ®µ
network_demo()


# ============================================================================
# 4. æ•°æ®åŠ è½½å™¨ï¼ˆå®Œæ•´å¯è¿è¡Œï¼‰
# ============================================================================

def dataloader_demo():
    print("\n" + "=" * 60)
    print("4. æ•°æ®åŠ è½½å™¨")
    print("=" * 60)

    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    num_samples = 100
    input_size = 5
    X = torch.randn(num_samples, input_size)
    y = torch.randint(0, 3, (num_samples,))  # 3ä¸ªç±»åˆ«

    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    dataset = TensorDataset(X, y)
    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)

    print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
    print(f"æ‰¹æ¬¡æ•°é‡: {len(dataloader)}")
    print(f"æ‰¹æ¬¡å¤§å°: {dataloader.batch_size}")

    # æŸ¥çœ‹ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
    for batch_idx, (data, target) in enumerate(dataloader):
        print(f"\nç¬¬ä¸€ä¸ªæ‰¹æ¬¡:")
        print(f"æ•°æ®å½¢çŠ¶: {data.shape}")
        print(f"æ ‡ç­¾å½¢çŠ¶: {target.shape}")
        print(f"æ•°æ®ç¤ºä¾‹: {data[0]}")
        print(f"æ ‡ç­¾ç¤ºä¾‹: {target[0]}")
        break  # åªçœ‹ç¬¬ä¸€ä¸ªæ‰¹æ¬¡


# è¿è¡Œç¬¬å››æ®µ
dataloader_demo()


# ============================================================================
# 5. å®Œæ•´è®­ç»ƒæµç¨‹ï¼ˆå®Œæ•´å¯è¿è¡Œï¼‰
# ============================================================================

def complete_training_demo():
    print("\n" + "=" * 60)
    print("5. å®Œæ•´è®­ç»ƒæµç¨‹")
    print("=" * 60)

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)

    # ç”Ÿæˆç®€å•çš„äºŒåˆ†ç±»æ•°æ®
    num_samples = 200
    X = torch.randn(num_samples, 2)
    # åˆ›å»ºç®€å•çš„äºŒåˆ†ç±»é—®é¢˜ï¼ˆæ ¹æ®ç‚¹çš„ä½ç½®ï¼‰
    y = ((X[:, 0] > 0) & (X[:, 1] > 0)).long()

    # åˆ†å‰²æ•°æ®é›†
    train_size = int(0.8 * num_samples)
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]

    # æ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(X_train, y_train)
    test_dataset = TensorDataset(X_test, y_test)

    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

    # å®šä¹‰ç®€å•æ¨¡å‹
    class BinaryClassifier(nn.Module):
        def __init__(self):
            super(BinaryClassifier, self).__init__()
            self.fc1 = nn.Linear(2, 10)
            self.fc2 = nn.Linear(10, 2)  # äºŒåˆ†ç±»ï¼Œè¾“å‡º2ä¸ªç±»åˆ«
            self.relu = nn.ReLU()

        def forward(self, x):
            x = self.relu(self.fc1(x))
            x = self.fc2(x)
            return x

    # åˆ›å»ºæ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨
    model = BinaryClassifier()
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.01)

    # è®­ç»ƒå‚æ•°
    num_epochs = 100
    train_losses = []
    train_accuracies = []

    print("å¼€å§‹è®­ç»ƒ...")

    for epoch in range(num_epochs):
        # è®­ç»ƒæ¨¡å¼
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for data, target in train_loader:
            # æ¢¯åº¦æ¸…é›¶
            optimizer.zero_grad()

            # å‰å‘ä¼ æ’­
            outputs = model(data)
            loss = criterion(outputs, target)

            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()

            # ç»Ÿè®¡
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

        # è®¡ç®—epochç»Ÿè®¡
        epoch_loss = running_loss / len(train_loader)
        epoch_acc = 100 * correct / total

        train_losses.append(epoch_loss)
        train_accuracies.append(epoch_acc)

        # æ¯5ä¸ªepochæ‰“å°ä¸€æ¬¡
        if (epoch + 1) % 5 == 0:
            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.1f}%')

    # æœ€ç»ˆæµ‹è¯•
    model.eval()
    test_correct = 0
    test_total = 0

    with torch.no_grad():
        for data, target in test_loader:
            outputs = model(data)
            _, predicted = torch.max(outputs.data, 1)
            test_total += target.size(0)
            test_correct += (predicted == target).sum().item()

    test_accuracy = 100 * test_correct / test_total
    print(f"\næœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.1f}%")

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 2, 1)
    plt.plot(train_losses, 'b-', linewidth=2)
    plt.title('è®­ç»ƒæŸå¤±')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True, alpha=0.3)

    plt.subplot(1, 2, 2)
    plt.plot(train_accuracies, 'g-', linewidth=2)
    plt.title('è®­ç»ƒå‡†ç¡®ç‡')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy (%)')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    return model, train_losses, train_accuracies


# è¿è¡Œç¬¬äº”æ®µï¼ˆè®­ç»ƒè¿‡ç¨‹ï¼‰
model, losses, accuracies = complete_training_demo()


# ============================================================================
# 6. æ¨¡å‹ä¿å­˜ä¸åŠ è½½ï¼ˆå®Œæ•´å¯è¿è¡Œï¼‰
# ============================================================================

def save_load_demo():
    print("\n" + "=" * 60)
    print("6. æ¨¡å‹ä¿å­˜ä¸åŠ è½½")
    print("=" * 60)

    # åˆ›å»ºç®€å•æ¨¡å‹
    class SimpleModel(nn.Module):
        def __init__(self):
            super(SimpleModel, self).__init__()
            self.fc = nn.Linear(5, 3)

        def forward(self, x):
            return self.fc(x)

    # åˆ›å»ºæ¨¡å‹å®ä¾‹å¹¶è®­ç»ƒä¸€ä¸‹ï¼ˆç®€å•æ¼”ç¤ºï¼‰
    model = SimpleModel()
    x = torch.randn(10, 5)
    y = torch.randint(0, 3, (10,))
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=0.01)

    # ä¸€æ¬¡è®­ç»ƒæ­¥éª¤ï¼ˆä¸ºäº†æœ‰å‚æ•°å¯ä¿å­˜ï¼‰
    optimizer.zero_grad()
    output = model(x)
    loss = criterion(output, y)
    loss.backward()
    optimizer.step()

    # ä¿å­˜æ¨¡å‹
    torch.save(model.state_dict(), 'model_weights.pth')
    print("æ¨¡å‹æƒé‡å·²ä¿å­˜åˆ° 'model_weights.pth'")

    # åŠ è½½æ¨¡å‹
    new_model = SimpleModel()
    new_model.load_state_dict(torch.load('model_weights.pth'))
    print("æ¨¡å‹æƒé‡å·²åŠ è½½")

    # éªŒè¯åŠ è½½çš„æ¨¡å‹
    test_input = torch.randn(1, 5)
    original_output = model(test_input)
    loaded_output = new_model(test_input)

    print(f"åŸå§‹æ¨¡å‹è¾“å‡º: {original_output.detach().numpy()}")
    print(f"åŠ è½½æ¨¡å‹è¾“å‡º: {loaded_output.detach().numpy()}")
    print("è¾“å‡ºæ˜¯å¦æ¥è¿‘:", torch.allclose(original_output, loaded_output, atol=1e-6))


# è¿è¡Œç¬¬å…­æ®µ
save_load_demo()


# ============================================================================
# 7. GPUä½¿ç”¨æ¼”ç¤ºï¼ˆå®Œæ•´å¯è¿è¡Œï¼‰
# ============================================================================

def gpu_demo():
    print("\n" + "=" * 60)
    print("7. GPUä½¿ç”¨æ¼”ç¤º")
    print("=" * 60)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»ºå¼ é‡
    x = torch.randn(3, 3)
    print(f"åŸå§‹å¼ é‡è®¾å¤‡: CPU")

    if torch.cuda.is_available():
        # ç§»åŠ¨åˆ°GPU
        x_gpu = x.to(device)
        print(f"GPUå¼ é‡è®¾å¤‡: {x_gpu.device}")

        # GPUè¿ç®—
        y_gpu = torch.matmul(x_gpu, x_gpu.t())
        print(f"GPUè¿ç®—ç»“æœå½¢çŠ¶: {y_gpu.shape}")

        # ç§»å›CPU
        y_cpu = y_gpu.cpu()
        print(f"ç§»å›CPUåçš„è®¾å¤‡: {y_cpu.device}")
    else:
        print("CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPUå®Œæˆæ¼”ç¤º")
        y_cpu = torch.matmul(x, x.t())
        print(f"CPUè¿ç®—ç»“æœå½¢çŠ¶: {y_cpu.shape}")


# è¿è¡Œç¬¬ä¸ƒæ®µ
gpu_demo()


# ============================================================================
# 8. çº¿æ€§å›å½’å®æˆ˜ï¼ˆå®Œæ•´å¯è¿è¡Œï¼‰
# ============================================================================

def linear_regression_demo():
    print("\n" + "=" * 60)
    print("8. çº¿æ€§å›å½’å®æˆ˜")
    print("=" * 60)

    # ç”Ÿæˆæ•°æ®
    torch.manual_seed(42)
    X = torch.linspace(-1, 1, 100).reshape(-1, 1)
    true_w = 2.0
    true_b = 1.0
    y = true_w * X + true_b + 0.1 * torch.randn(X.size())

    # å®šä¹‰çº¿æ€§å›å½’æ¨¡å‹
    class LinearRegression(nn.Module):
        def __init__(self):
            super(LinearRegression, self).__init__()
            self.linear = nn.Linear(1, 1)

        def forward(self, x):
            return self.linear(x)

    model = LinearRegression()
    criterion = nn.MSELoss()
    optimizer = optim.SGD(model.parameters(), lr=0.01)

    # è®­ç»ƒæ¨¡å‹
    losses = []
    for epoch in range(3000):
        # å‰å‘ä¼ æ’­
        outputs = model(X)
        loss = criterion(outputs, y)

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        losses.append(loss.item())

        if epoch % 50 == 0:
            print(f'Epoch [{epoch}/100], Loss: {loss.item():.4f}')

    # è·å–è®­ç»ƒåçš„å‚æ•°
    w_pred = model.linear.weight.item()
    b_pred = model.linear.bias.item()

    print(f"\nçœŸå®å‚æ•°: w = {true_w:.3f}, b = {true_b:.3f}")
    print(f"é¢„æµ‹å‚æ•°: w = {w_pred:.3f}, b = {b_pred:.3f}")

    # ç»˜åˆ¶ç»“æœ
    plt.figure(figsize=(10, 4))

    plt.subplot(1, 2, 1)
    plt.scatter(X.numpy(), y.numpy(), alpha=0.7, label='æ•°æ®ç‚¹')
    plt.plot(X.numpy(), model(X).detach().numpy(), 'r-', linewidth=2, label='æ‹Ÿåˆç›´çº¿')
    plt.xlabel('X')
    plt.ylabel('y')
    plt.legend()
    plt.title('çº¿æ€§å›å½’æ‹Ÿåˆ')
    plt.grid(True, alpha=0.3)

    plt.subplot(1, 2, 2)
    plt.plot(losses)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('è®­ç»ƒæŸå¤±')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()


# è¿è¡Œç¬¬å…«æ®µ
linear_regression_demo()

print("\nğŸ‰ æ‰€æœ‰ä»£ç æ®µéƒ½æˆåŠŸè¿è¡Œå®Œæˆï¼")
print("âœ… ä½ å·²ç»æŒæ¡äº†PyTorchçš„æ ¸å¿ƒåŠŸèƒ½")
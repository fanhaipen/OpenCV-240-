"""
æ¢¯åº¦ä¸‹é™çš„ä¸‰ç§å˜ä½“å¯¹æ¯” - ä¿®å¤ç‰ˆ
1. æ‰¹é‡æ¢¯åº¦ä¸‹é™ (Batch Gradient Descent)
2. éšæœºæ¢¯åº¦ä¸‹é™ (Stochastic Gradient Descent)
3. å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ (Mini-batch Gradient Descent)
"""

import numpy as np
import matplotlib.pyplot as plt

print("=== æ¢¯åº¦ä¸‹é™çš„ä¸‰ç§å˜ä½“å¯¹æ¯” ===")
print()

# 1. åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
np.random.seed(42)
x_train = np.array([1.0, 2.0, 3.0, 4.0, 5.0])
y_train = 2 * x_train + 1 + np.random.randn(5) * 0.5  # y = 2x + 1 + å™ªå£°

print("ğŸ“Š è®­ç»ƒæ•°æ®ï¼š")
for i, (x, y) in enumerate(zip(x_train, y_train), 1):
    print(f"  ç‚¹{i}: x={x:.1f}, çœŸå®y={y:.2f}, ç›®æ ‡y={2*x+1:.1f}")

# 2. åˆå§‹åŒ–å‚æ•°
w_init, b_init = 0.0, 0.0
lr = 0.01
n_samples = len(x_train)

print(f"\nğŸ¯ ç›®æ ‡ï¼šæ‰¾åˆ° y = w*x + b ä¸­çš„ w å’Œ b")
print(f"ğŸ¤” åˆå§‹çŒœæµ‹ï¼šw={w_init}, b={b_init}")
print(f"ğŸ“ˆ çœŸå®å€¼ï¼šwâ‰ˆ2.0, bâ‰ˆ1.0")
print()

# 3. å®šä¹‰æŸå¤±å‡½æ•°
def compute_loss(w, b, x, y):
    """è®¡ç®—å‡æ–¹è¯¯å·®æŸå¤±"""
    y_pred = w * x + b
    loss = np.mean((y_pred - y) ** 2)
    return loss

# 4. æ‰¹é‡æ¢¯åº¦ä¸‹é™ (Batch Gradient Descent) - ä¿®å¤ï¼šè¿è¡Œå®Œæ•´3ä¸ªepoch
print("="*60)
print("1. æ‰¹é‡æ¢¯åº¦ä¸‹é™ (Batch Gradient Descent)")
print("-"*60)
print("ğŸ“Œ ç‰¹ç‚¹ï¼šä¸€æ¬¡ç”¨æ‰€æœ‰æ•°æ®è®¡ç®—æ¢¯åº¦")
print("âœ… ä¼˜ç‚¹ï¼šæ¢¯åº¦å‡†ç¡®ï¼Œæ–¹å‘ç¨³å®š")
print("âŒ ç¼ºç‚¹ï¼šå¤§æ•°æ®é›†æ—¶å¤ªæ…¢")
print()

def batch_gradient_descent(w, b, x, y, lr=0.01, epochs=3):
    """æ‰¹é‡æ¢¯åº¦ä¸‹é™ - æ¯æ¬¡ç”¨å…¨éƒ¨æ•°æ®æ›´æ–°"""
    w_history, b_history, loss_history = [w], [b], [compute_loss(w, b, x, y)]

    for epoch in range(epochs):
        print(f"\nğŸ“… Epoch {epoch+1}:")

        # 1. ç”¨å½“å‰å‚æ•°é¢„æµ‹æ‰€æœ‰ç‚¹
        y_pred = w * x + b
        errors = y_pred - y

        # 2. è®¡ç®—æ¢¯åº¦ï¼ˆç”¨æ‰€æœ‰æ•°æ®çš„å¹³å‡å€¼ï¼‰
        w_grad = 2 * np.mean(errors * x)
        b_grad = 2 * np.mean(errors)

        # 3. æ›´æ–°å‚æ•°
        w = w - lr * w_grad
        b = b - lr * b_grad

        # 4. è®°å½•
        w_history.append(w)
        b_history.append(b)
        loss_history.append(compute_loss(w, b, x, y))

    return w, b, w_history, b_history, loss_history

# è¿è¡Œæ‰¹é‡æ¢¯åº¦ä¸‹é™
w_batch, b_batch, w_hist_batch, b_hist_batch, loss_hist_batch = batch_gradient_descent(
    w_init, b_init, x_train, y_train, lr, epochs=3
)

# 5. éšæœºæ¢¯åº¦ä¸‹é™ (SGD) - ä¿®å¤ï¼šè¿è¡Œå®Œæ•´1ä¸ªepochï¼ˆ5ä¸ªç‚¹ï¼‰
print("\n" + "="*60)
print("2. éšæœºæ¢¯åº¦ä¸‹é™ (Stochastic Gradient Descent)")
print("-"*60)
print("ğŸ“Œ ç‰¹ç‚¹ï¼šæ¯æ¬¡ç”¨1ä¸ªæ•°æ®ç‚¹è®¡ç®—æ¢¯åº¦")
print("âœ… ä¼˜ç‚¹ï¼šæ›´æ–°å¿«ï¼Œèƒ½è·³å‡ºå±€éƒ¨æœ€ä¼˜")
print("âŒ ç¼ºç‚¹ï¼šä¸ç¨³å®šï¼Œæœ‰å™ªå£°")
print()

def stochastic_gradient_descent(w, b, x, y, lr=0.01, epochs=1):
    """éšæœºæ¢¯åº¦ä¸‹é™ - æ¯æ¬¡ç”¨1ä¸ªæ•°æ®ç‚¹æ›´æ–°"""
    w_history, b_history, loss_history = [w], [b], [compute_loss(w, b, x, y)]
    n = len(x)

    for epoch in range(epochs):
        # éšæœºæ‰“ä¹±æ•°æ®
        indices = np.random.permutation(n)
        x_shuffled = x[indices]
        y_shuffled = y[indices]

        for i, (xi, yi) in enumerate(zip(x_shuffled, y_shuffled), 1):
            # ç”¨è¿™ä¸€ä¸ªç‚¹è®¡ç®—æ¢¯åº¦
            error = (w * xi + b) - yi
            w_grad = 2 * error * xi
            b_grad = 2 * error

            # æ›´æ–°å‚æ•°
            w = w - lr * w_grad
            b = b - lr * b_grad

            # è®°å½•
            w_history.append(w)
            b_history.append(b)
            loss_history.append(compute_loss(w, b, x, y))

    return w, b, w_history, b_history, loss_history

# è¿è¡Œéšæœºæ¢¯åº¦ä¸‹é™
w_sgd, b_sgd, w_hist_sgd, b_hist_sgd, loss_hist_sgd = stochastic_gradient_descent(
    w_init, b_init, x_train, y_train, lr, epochs=1
)

# 6. å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ (Mini-batch GD) - ä¿®å¤ï¼šè¿è¡Œå®Œæ•´1ä¸ªepoch
print("\n" + "="*60)
print("3. å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ (Mini-batch Gradient Descent)")
print("-"*60)
print("ğŸ“Œ ç‰¹ç‚¹ï¼šä¸€æ¬¡ç”¨ä¸€å°æ‰¹æ•°æ®ï¼ˆbatch_size=2ï¼‰")
print("âœ… ä¼˜ç‚¹ï¼šå¹³è¡¡é€Ÿåº¦å’Œç¨³å®šæ€§")
print("ğŸ’¡ æœ€å¸¸ç”¨ï¼Œæ·±åº¦å­¦ä¹ æ ‡é…")
print()

def minibatch_gradient_descent(w, b, x, y, lr=0.01, batch_size=2, epochs=1):
    """å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ - æ¯æ¬¡ç”¨batch_sizeä¸ªæ•°æ®æ›´æ–°"""
    w_history, b_history, loss_history = [w], [b], [compute_loss(w, b, x, y)]
    n = len(x)

    for epoch in range(epochs):
        # éšæœºæ‰“ä¹±æ•°æ®
        indices = np.random.permutation(n)
        x_shuffled = x[indices]
        y_shuffled = y[indices]

        # åˆ†æ‰¹å¤„ç†
        for batch_idx in range(0, n, batch_size):
            # è·å–ä¸€ä¸ªå°æ‰¹é‡
            batch_end = min(batch_idx + batch_size, n)
            x_batch = x_shuffled[batch_idx:batch_end]
            y_batch = y_shuffled[batch_idx:batch_end]

            # ç”¨è¿™ä¸ªbatchè®¡ç®—å¹³å‡æ¢¯åº¦
            errors = (w * x_batch + b) - y_batch
            w_grad = 2 * np.mean(errors * x_batch)
            b_grad = 2 * np.mean(errors)

            # æ›´æ–°å‚æ•°
            w = w - lr * w_grad
            b = b - lr * b_grad

            # è®°å½•
            w_history.append(w)
            b_history.append(b)
            loss_history.append(compute_loss(w, b, x, y))

    return w, b, w_history, b_history, loss_history

# è¿è¡Œå°æ‰¹é‡æ¢¯åº¦ä¸‹é™
batch_size = 2
w_mini, b_mini, w_hist_mini, b_hist_mini, loss_hist_mini = minibatch_gradient_descent(
    w_init, b_init, x_train, y_train, lr, batch_size=batch_size, epochs=1
)

# 7. å¯è§†åŒ–å¯¹æ¯” - ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰æ›²çº¿éƒ½æ˜¾ç¤º
print("\n" + "="*60)
print("ğŸ“ˆ ä¸‰ç§æ–¹æ³•çš„å¯è§†åŒ–å¯¹æ¯”")
print("-"*60)

fig, axes = plt.subplots(2, 3, figsize=(15, 10))

# è®¾ç½®ä¸åŒçº¿å‹å’Œæ ‡è®°ï¼Œç¡®ä¿åŒºåˆ†åº¦
batch_style = {'color': 'blue', 'marker': 'o', 'linestyle': '-', 'linewidth': 2, 'markersize': 6}
sgd_style = {'color': 'red', 'marker': 's', 'linestyle': '--', 'linewidth': 2, 'markersize': 6}
mini_style = {'color': 'green', 'marker': '^', 'linestyle': '-.', 'linewidth': 2, 'markersize': 8}

# 1. æŸå¤±å˜åŒ–å¯¹æ¯”
ax1 = axes[0, 0]

# è®¡ç®—æ¯ä¸ªæ–¹æ³•çš„æ›´æ–°æ¬¡æ•°
batch_updates = len(loss_hist_batch) - 1
sgd_updates = len(loss_hist_sgd) - 1
mini_updates = len(loss_hist_mini) - 1

# æ‰¹é‡GDçš„æŸå¤±
ax1.plot(range(len(loss_hist_batch)), loss_hist_batch,
         label=f'Batch GD (æ›´æ–°={batch_updates})', **batch_style)

# éšæœºGDçš„æŸå¤±
ax1.plot(range(len(loss_hist_sgd)), loss_hist_sgd,
         label=f'SGD (æ›´æ–°={sgd_updates})', **sgd_style)

# å°æ‰¹é‡GDçš„æŸå¤±
ax1.plot(range(len(loss_hist_mini)), loss_hist_mini,
         label=f'Mini-batch GD (æ›´æ–°={mini_updates})', **mini_style)

ax1.set_xlabel('æ›´æ–°æ¬¡æ•°')
ax1.set_ylabel('æŸå¤±')
ax1.set_title('ä¸‰ç§æ–¹æ³•çš„æŸå¤±å˜åŒ–')
ax1.legend()
ax1.grid(True, alpha=0.3)
# ä½¿ç”¨çº¿æ€§åæ ‡è€Œä¸æ˜¯å¯¹æ•°åæ ‡ï¼Œè®©æ›²çº¿æ›´æ˜æ˜¾
# ax1.set_yscale('log')

# 2. å‚æ•°wçš„å˜åŒ–
ax2 = axes[0, 1]

# æ‰¹é‡GD
ax2.plot(range(len(w_hist_batch)), w_hist_batch,
         label=f'Batch GD (epoch=3)', **batch_style)
# éšæœºGD
ax2.plot(range(len(w_hist_sgd)), w_hist_sgd,
         label=f'SGD (5ä¸ªç‚¹)', **sgd_style)
# å°æ‰¹é‡GD
ax2.plot(range(len(w_hist_mini)), w_hist_mini,
         label=f'Mini-batch GD (batch_size={batch_size})', **mini_style)

# æ·»åŠ ç›®æ ‡çº¿
ax2.axhline(y=2.0, color='k', linestyle=':', alpha=0.7, label='ç›®æ ‡w=2.0')
ax2.set_xlabel('æ›´æ–°æ¬¡æ•°')
ax2.set_ylabel('wå€¼')
ax2.set_title('å‚æ•°wçš„å˜åŒ–')
ax2.legend()
ax2.grid(True, alpha=0.3)

# 3. å‚æ•°bçš„å˜åŒ–
ax3 = axes[0, 2]

# æ‰¹é‡GD
ax3.plot(range(len(b_hist_batch)), b_hist_batch,
         label='Batch GD', **batch_style)
# éšæœºGD
ax3.plot(range(len(b_hist_sgd)), b_hist_sgd,
         label='SGD', **sgd_style)
# å°æ‰¹é‡GD
ax3.plot(range(len(b_hist_mini)), b_hist_mini,
         label='Mini-batch GD', **mini_style)

# æ·»åŠ ç›®æ ‡çº¿
ax3.axhline(y=1.0, color='k', linestyle=':', alpha=0.7, label='ç›®æ ‡b=1.0')
ax3.set_xlabel('æ›´æ–°æ¬¡æ•°')
ax3.set_ylabel('bå€¼')
ax3.set_title('å‚æ•°bçš„å˜åŒ–')
ax3.legend()
ax3.grid(True, alpha=0.3)

# 4. å‚æ•°ç©ºé—´ä¸­çš„è·¯å¾„
ax4 = axes[1, 0]

# æ‰¹é‡GDè·¯å¾„
ax4.plot(w_hist_batch, b_hist_batch, label='Batch GD',
         **batch_style)
# éšæœºGDè·¯å¾„
ax4.plot(w_hist_sgd, b_hist_sgd, label='SGD',
         **sgd_style)
# å°æ‰¹é‡GDè·¯å¾„
ax4.plot(w_hist_mini, b_hist_mini, label='Mini-batch GD',
         **mini_style)

# æ ‡è®°èµ·ç‚¹
ax4.plot(w_hist_batch[0], b_hist_batch[0], 'ko', markersize=10, label='èµ·ç‚¹')
# æ ‡è®°ç›®æ ‡
ax4.plot(2.0, 1.0, 'y*', markersize=15, label='ç›®æ ‡ç‚¹')

ax4.set_xlabel('wå€¼')
ax4.set_ylabel('bå€¼')
ax4.set_title('å‚æ•°ç©ºé—´ä¸­çš„æ›´æ–°è·¯å¾„')
ax4.legend()
ax4.grid(True, alpha=0.3)

# 5. å¯¹æ¯”è¡¨æ ¼
ax5 = axes[1, 1]
ax5.axis('off')

# è®¡ç®—æœ€ç»ˆæŸå¤±
final_loss_batch = compute_loss(w_batch, b_batch, x_train, y_train)
final_loss_sgd = compute_loss(w_sgd, b_sgd, x_train, y_train)
final_loss_mini = compute_loss(w_mini, b_mini, x_train, y_train)

table_data = [
    ['æ–¹æ³•', 'æ›´æ–°æ¬¡æ•°', 'æœ€ç»ˆw', 'æœ€ç»ˆb', 'æœ€ç»ˆæŸå¤±'],
    ['Batch GD', f'{batch_updates}', f'{w_batch:.4f}', f'{b_batch:.4f}', f'{final_loss_batch:.6f}'],
    ['SGD', f'{sgd_updates}', f'{w_sgd:.4f}', f'{b_sgd:.4f}', f'{final_loss_sgd:.6f}'],
    ['Mini-batch', f'{mini_updates}', f'{w_mini:.4f}', f'{b_mini:.4f}', f'{final_loss_mini:.6f}']
]

table = ax5.table(cellText=table_data, loc='center', cellLoc='center',
                  colWidths=[0.2, 0.2, 0.2, 0.2, 0.2])
table.auto_set_font_size(False)
table.set_fontsize(12)
table.scale(1.2, 2)
ax5.set_title(f'ä¸‰ç§æ–¹æ³•å¯¹æ¯” (batch_size={batch_size})')

# 6. æ¯”å–»è¯´æ˜
ax6 = axes[1, 2]
ax6.axis('off')

text = """
ğŸ¯ ä¸‰ç§æ¢¯åº¦ä¸‹é™æ–¹æ³•å¯¹æ¯”ï¼š

1. æ‰¹é‡æ¢¯åº¦ä¸‹é™ (Batch GD)
   ğŸ‘¨â€ğŸ”¬ ç§‘å­¦å®¶æ–¹æ³•
   - æ¯æ¬¡ç”¨å…¨éƒ¨æ•°æ®
   - æ›´æ–°æ¬¡æ•° = epochæ•°
   - ç¨³å®šä½†æ…¢

2. éšæœºæ¢¯åº¦ä¸‹é™ (SGD)
   ğŸƒ å†’é™©å®¶æ–¹æ³•
   - æ¯æ¬¡ç”¨1ä¸ªæ•°æ®
   - æ›´æ–°æ¬¡æ•° = æ•°æ®ç‚¹æ•°
   - å¿«é€Ÿä½†æ³¢åŠ¨å¤§

3. å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ (Mini-batch)
   ğŸ‘¥ å›¢é˜Ÿæ–¹æ³•
   - æ¯æ¬¡ç”¨ä¸€æ‰¹æ•°æ®
   - æ›´æ–°æ¬¡æ•° = æ‰¹æ¬¡æ•°
   - å¹³è¡¡é€Ÿåº¦ä¸ç¨³å®š
   - æ·±åº¦å­¦ä¹ æœ€å¸¸ç”¨

ğŸ“Š æœ¬å®éªŒè®¾ç½®ï¼š
   - æ•°æ®ç‚¹ï¼š5ä¸ª
   - batch_sizeï¼š2
   - Batch GDï¼š3æ¬¡æ›´æ–°
   - SGDï¼š5æ¬¡æ›´æ–°
   - Mini-batchï¼š3æ¬¡æ›´æ–°
"""

ax6.text(0.1, 0.5, text, fontsize=10,
         verticalalignment='center',
         transform=ax6.transAxes)
ax6.set_title('æ–¹æ³•æ€»ç»“')

plt.tight_layout()
plt.savefig('gradient_descent_variants_fixed.png', dpi=100, bbox_inches='tight')
print("âœ… å›¾è¡¨å·²ä¿å­˜ä¸º gradient_descent_variants_fixed.png")
print()

# 8. è¯¦ç»†å¯¹æ¯”
print("="*60)
print("ğŸ“Š è¯¦ç»†å¯¹æ¯”")
print("-"*60)
print(f"æ•°æ®ç‚¹æ•°é‡: {len(x_train)}")
print(f"Batch GD: æ¯ä¸ªepochæ›´æ–°1æ¬¡ï¼Œ{len(loss_hist_batch)-1}æ¬¡æ›´æ–°")
print(f"SGD: æ¯ä¸ªç‚¹æ›´æ–°1æ¬¡ï¼Œ{len(loss_hist_sgd)-1}æ¬¡æ›´æ–°")
print(f"Mini-batch (batch_size={batch_size}): æ¯æ‰¹æ›´æ–°1æ¬¡ï¼Œ{len(loss_hist_mini)-1}æ¬¡æ›´æ–°")
print()

print("ğŸ” ä¸ºä»€ä¹ˆæœ‰çš„æ›²çº¿ä¸æ˜¾ç¤ºï¼Ÿ")
print("-"*30)
print("1. æ›´æ–°æ¬¡æ•°ä¸åŒï¼šä¸‰ç§æ–¹æ³•çš„æ›´æ–°é¢‘ç‡ä¸åŒ")
print("2. æ¨ªåæ ‡ä¸å¯¹é½ï¼šBatch GDæ¯ä¸ªepochä¸€æ¬¡ï¼ŒSGDæ¯ä¸ªç‚¹ä¸€æ¬¡")
print("3. æŸå¤±å€¼å·®å¼‚ï¼šä¸åŒæ–¹æ³•çš„æŸå¤±å˜åŒ–å¹…åº¦ä¸åŒ")
print("4. ç»˜å›¾è®¾ç½®ï¼šå¯èƒ½çº¿å‹ã€é¢œè‰²ã€æ ‡è®°å¤§å°è®¾ç½®ä¸å½“")
print()

print("âœ¨ ä¿®å¤æªæ–½ï¼š")
print("-"*30)
print("1. ç»Ÿä¸€æ˜¾ç¤ºæ‰€æœ‰æ–¹æ³•çš„å®Œæ•´è®­ç»ƒè¿‡ç¨‹")
print("2. ä½¿ç”¨ä¸åŒçº¿å‹ï¼ˆå®çº¿ã€è™šçº¿ã€ç‚¹åˆ’çº¿ï¼‰")
print("3. è°ƒæ•´æ ‡è®°å¤§å°å’Œé¢œè‰²å¯¹æ¯”åº¦")
print("4. ä½¿ç”¨çº¿æ€§åæ ‡è€Œéå¯¹æ•°åæ ‡")
print()

# 9. æµ‹è¯•ä¸åŒbatch_size
print("="*60)
print("ğŸ® æµ‹è¯•ä¸åŒbatch_sizeçš„æ•ˆæœ")
print("-"*60)

def test_batch_size(batch_size, epochs=3, lr=0.01):
    """æµ‹è¯•ä¸åŒbatch_sizeçš„æ•ˆæœ"""
    w, b = w_init, b_init
    n = len(x_train)
    loss_history = []

    for epoch in range(epochs):
        indices = np.random.permutation(n)
        x_shuffled = x_train[indices]
        y_shuffled = y_train[indices]

        for i in range(0, n, batch_size):
            batch_end = min(i + batch_size, n)
            x_batch = x_shuffled[i:batch_end]
            y_batch = y_shuffled[i:batch_end]

            # è®¡ç®—æ¢¯åº¦
            errors = (w * x_batch + b) - y_batch
            w_grad = 2 * np.mean(errors * x_batch)
            b_grad = 2 * np.mean(errors)

            # æ›´æ–°å‚æ•°
            w -= lr * w_grad
            b -= lr * b_grad

            # è®°å½•æŸå¤±
            loss_history.append(compute_loss(w, b, x_train, y_train))

    updates = len(loss_history)
    final_loss = loss_history[-1] if loss_history else compute_loss(w, b, x_train, y_train)

    return w, b, updates, final_loss, loss_history

# æµ‹è¯•ä¸åŒbatch_size
batch_sizes = [1, 2, 3, 5]
results = []

for bs in batch_sizes:
    w_final, b_final, updates, final_loss, _ = test_batch_size(bs, epochs=3, lr=0.01)
    results.append((bs, updates, w_final, b_final, final_loss))

print("ä¸åŒbatch_sizeçš„å¯¹æ¯”ï¼š")
print("batch_size | æ›´æ–°æ¬¡æ•° | æœ€ç»ˆw | æœ€ç»ˆb | æœ€ç»ˆæŸå¤±")
print("-" * 50)
for bs, updates, w, b, loss in results:
    if bs == 5:
        method = "Batch GD"
    elif bs == 1:
        method = "SGD"
    else:
        method = f"Mini-batch({bs})"

    print(f"{method:12} | {updates:8d} | {w:6.3f} | {b:6.3f} | {loss:8.6f}")

print("\n" + "="*60)
print("ğŸ¯ æ€»ç»“")
print("-"*60)
print("1. Batch GD (batch_size=å…¨éƒ¨æ•°æ®)")
print("   - æ›´æ–°æœ€ç¨³å®šï¼Œä½†æœ€æ…¢")
print("   - é€‚åˆå°æ•°æ®é›†")
print()
print("2. SGD (batch_size=1)")
print("   - æ›´æ–°æœ€å¿«ï¼Œä½†æœ€ä¸ç¨³å®š")
print("   - é€‚åˆå¤§æ•°æ®é›†")
print()
print("3. Mini-batch GD (batch_size=2-256)")
print("   - å¹³è¡¡é€Ÿåº¦å’Œç¨³å®šæ€§")
print("   - æ·±åº¦å­¦ä¹ æœ€å¸¸ç”¨")
print("   - batch_sizeé€šå¸¸ä¸º32, 64, 128")
print()
print("ğŸ’¡ å…³é”®ï¼šåœ¨PyTorchä¸­ï¼Œé€šè¿‡DataLoaderçš„batch_sizeå‚æ•°æ§åˆ¶ï¼")

plt.show()
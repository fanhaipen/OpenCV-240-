"""
Mini-batch æ¢¯åº¦ä¸‹é™è¯¦è§£
æ ¸å¿ƒæ€æƒ³ï¼šæ¯æ¬¡ç”¨ä¸€å°æ‰¹æ•°æ®è®¡ç®—æ¢¯åº¦ï¼Œå¹³è¡¡é€Ÿåº¦å’Œç¨³å®šæ€§
"""

import numpy as np
import matplotlib.pyplot as plt

# ç†è®ºåŸºç¡€
print("ğŸ¯ Mini-batch GD: å¹³è¡¡SGDé€Ÿåº¦å’ŒBatch GDç¨³å®šæ€§")
print("batch_sizeé€‰æ‹©: 1=SGD, n=Batch GD, 2-256=Mini-batch")

# åˆ›å»ºæ•°æ®
np.random.seed(42)
n_samples = 100
x = np.random.randn(n_samples, 1)
true_w, true_b = 3.0, 2.0
y = true_w * x + true_b + np.random.randn(n_samples, 1) * 0.5

# Mini-batch GDå®ç°
def train_gd(x, y, batch_size=32, lr=0.01, epochs=10):
    n = len(x)
    w, b = 0.0, 0.0
    loss_history = []

    for epoch in range(epochs):
        indices = np.random.permutation(n)
        for i in range(0, n, batch_size):
            batch_indices = indices[i:min(i+batch_size, n)]
            x_batch = x[batch_indices].flatten()
            y_batch = y[batch_indices].flatten()

            # è®¡ç®—æ¢¯åº¦
            errors = w * x_batch + b - y_batch
            w_grad = 2 * np.mean(errors * x_batch)
            b_grad = 2 * np.mean(errors)

            # æ›´æ–°å‚æ•°
            w -= lr * w_grad
            b -= lr * b_grad
            loss_history.append(np.mean(errors ** 2))

    return w, b, loss_history

# æµ‹è¯•ä¸åŒbatch_size
batch_sizes = [8, 16, 32, 128]
results = []
loss_data = []

for bs in batch_sizes:
    w, b, loss_hist = train_gd(x, y, bs, epochs=500)
    method = "SGD" if bs == 1 else "Batch GD" if bs == 100 else f"Mini-batch({bs})"
    results.append((method, bs, w, b, loss_hist[-1], len(loss_hist)))
    loss_data.append(loss_hist)

# ç»“æœæ˜¾ç¤º
print("\nğŸ“Š ç»“æœå¯¹æ¯”:")
print("æ–¹æ³•            | batch_size | æœ€ç»ˆw   | æœ€ç»ˆb   | æœ€ç»ˆæŸå¤±")
print("-"*60)
for method, bs, w, b, loss, _ in results:
    print(f"{method:15} | {bs:10} | {w:6.3f} | {b:6.3f} | {loss:8.6f}")

# å¯è§†åŒ–
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

colors = ['red', 'orange', 'green', 'blue']
labels = ['SGD (bs=1)', 'Mini-batch (bs=16)', 'Mini-batch (bs=32)', 'Batch GD (bs=100)']

# æŸå¤±æ›²çº¿å¯¹æ¯”
ax1 = axes[0,0]
for i, loss_hist in enumerate(loss_data):
    step = max(1, len(loss_hist) // 50)
    x_vals = range(0, len(loss_hist), step)
    y_vals = loss_hist[::step]
    ax1.plot(x_vals, y_vals, color=colors[i], label=labels[i], linewidth=2)
ax1.set_xlabel('æ›´æ–°æ¬¡æ•°')
ax1.set_ylabel('æŸå¤±')
ax1.legend()
ax1.grid(True, alpha=0.3)

# æ”¶æ•›é€Ÿåº¦å¯¹æ¯”
ax2 = axes[0,1]
threshold = 0.5
convergence_speed = []
for loss_hist in loss_data:  # ä¿®å¤è¿™é‡Œï¼šä»lossæ”¹ä¸ºloss_data
    found = False
    for j, loss_val in enumerate(loss_hist):
        if loss_val < threshold:
            convergence_speed.append(j)
            found = True
            break
    if not found:
        convergence_speed.append(len(loss_hist))

ax2.bar(labels, convergence_speed, color=colors, alpha=0.7)
ax2.set_xlabel('æ–¹æ³•')
ax2.set_ylabel('æ”¶æ•›é€Ÿåº¦')
ax2.tick_params(axis='x', rotation=45)
ax2.grid(True, alpha=0.3, axis='y')

# æ¢¯åº¦å™ªå£°å¯¹æ¯”
ax3 = axes[1,0]
noise_levels = []
for loss_hist in loss_data:  # ä¿®å¤è¿™é‡Œï¼šä»lossæ”¹ä¸ºloss_data
    if len(loss_hist) > 10:
        noise = np.std(loss_hist[-10:]) / np.mean(loss_hist[-10:])
        noise_levels.append(noise)
    else:
        noise_levels.append(0)
ax3.bar(labels, noise_levels, color=colors, alpha=0.7)
ax3.set_xlabel('æ–¹æ³•')
ax3.set_ylabel('æ¢¯åº¦å™ªå£°')
ax3.tick_params(axis='x', rotation=45)
ax3.grid(True, alpha=0.3, axis='y')

# æ›´æ–°é¢‘ç‡å¯¹æ¯”
ax4 = axes[1,1]
update_counts = [len(hist) for hist in loss_data]
ax4.bar(labels, update_counts, color=colors, alpha=0.7)
ax4.set_xlabel('æ–¹æ³•')
ax4.set_ylabel('æ€»æ›´æ–°æ¬¡æ•°')
ax4.tick_params(axis='x', rotation=45)
ax4.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

# æ€»ç»“
print("\nğŸ¯ æ ¸å¿ƒæ€»ç»“:")
print("1. SGD (bs=1): æ›´æ–°å¿«ä½†å™ªå£°å¤§")
print("2. Batch GD (bs=n): ç¨³å®šä½†æ”¶æ•›æ…¢")
print("3. Mini-batch (bs=32): æœ€ä½³å¹³è¡¡ç‚¹")
print("4. æ·±åº¦å­¦ä¹ å¸¸ç”¨: 32, 64, 128")

print("\nğŸ’» PyTorchç¤ºä¾‹:")
print("DataLoader(dataset, batch_size=32)  # Mini-batch GD")
print("DataLoader(dataset, batch_size=1)    # SGD")
print("DataLoader(dataset, batch_size=len(dataset))  # Batch GD")